{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|STT|\tHọ và tên|\tMSSV\t|Email|\n",
        "|-|--|--|--|\n",
        "|1|\tTrần Hoàng Khang\t|19521671|\t19521671@gm.uit.edu.vn|\n",
        "|2|\tNguyễn Tú Ngọc\t|20521665\t|20521665@gm.uit.edu.vn|\n",
        "|3 |\tLê Hồng Bằng\t|19520396\t|19520396@gm.uit.edu.vn|\n",
        "\n"
      ],
      "metadata": {
        "id": "pQKuQHVhNEd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Official documentation OpenAI Gym: https://www.gymlibrary.dev/environments/toy_text/taxi/\n",
        "\n",
        "**Taxi-v3**: Map Correction + Cleaner Domain Description, v0.25.0 action masking added to the reset and step information"
      ],
      "metadata": {
        "id": "7rFjRiqsRHfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ONKo0W3M8h",
        "outputId": "0a7f2317-ba82-4a7f-a9d2-530fda10c530"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. General view"
      ],
      "metadata": {
        "id": "L3hGaVh4Prf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q49teq5pLH_m",
        "outputId": "ddf59e95-0318-4c39-bf4d-5c6cb93f17f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[toy_text]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[toy_text]) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.12.1+cu113)\n",
            "Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (4.13.0)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (0.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (4.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable_baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2022.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install gym[toy_text]\n",
        "! pip install stable_baselines3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "8K03tNGk_Zoh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "save_video_path = '/content/drive/MyDrive/Lab/Ex2/video'\n",
        "\n",
        "# create Taxi environment and prepare the path to store output video\n",
        "env = Monitor(gym.make('Taxi-v3'), save_video_path, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# There are many JSON file about metadata of an episodes\n",
        "# There is 1 file storing the map status, you can consider it as a video\n",
        "\n",
        "f = open('/content/video/openaigym.video.1.76.video000000.json', 'r')\n",
        "file = f.read()\n",
        "jsonObj = json.loads(file)\n",
        "print(jsonObj['stdout'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDBD0YH37Ltv",
        "outputId": "ca09f07e-6448-420d-e0d0-0ac4746436dc"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : :\\x1b[43m \\x1b[0m|\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : :\\x1b[43m \\x1b[0m|\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : :\\x1b[43m \\x1b[0m|\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : :\\x1b[43m \\x1b[0m|\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | :\\x1b[43m \\x1b[0m|\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : |\\x1b[43m \\x1b[0m: |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : |\\x1b[43m \\x1b[0m: |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : :\\x1b[43m \\x1b[0m: |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : :\\x1b[43m \\x1b[0m: |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : :\\x1b[43m \\x1b[0m: |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : :\\x1b[43m \\x1b[0m|\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | :\\x1b[43m \\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : |\\x1b[42m_\\x1b[0m: : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | :\\x1b[42m_\\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | : : |\\r\\n| : : :\\x1b[42m_\\x1b[0m: |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | :\\x1b[42m_\\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | : :\\x1b[42m_\\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[42m_\\x1b[0m: :G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | :\\x1b[42m_\\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :G|\\r\\n| : | :\\x1b[42m_\\x1b[0m: |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[42m_\\x1b[0m:G|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[42mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : :\\x1b[43m \\x1b[0m|\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (South)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (North)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Pickup)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (Dropoff)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (East)\\r\\n'], [0.5, '\\x1b[2J\\x1b[1;1H+---------+\\r\\n|R: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\r\\n| : | : : |\\r\\n| : : : : |\\r\\n| | : | : |\\r\\n|\\x1b[35mY\\x1b[0m| : |B: |\\r\\n+---------+\\r\\n  (West)\\r\\n']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "- There are four designated locations in the grid world indicated by `R(ed)`, `G(reen)`, `Y(ellow)`, and `B(lue)`. \n",
        "- When the episode starts, the taxi starts off at a **random square** and the passenger is at a **random location** (R/G/Y/B).\n",
        "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "- The taxi drives to the passenger’s location -> *picks up the passenger*, *drives to the passenger’s destination* (another one of the four specified locations) -> *drops off the passenger*. \n",
        "- Once the passenger is dropped off -> the episode ends.\n",
        "\n",
        "<pre>\n",
        "+---------+\n",
        "|R: | : :G|\n",
        "| : | : : |\n",
        "| : : : : |\n",
        "| | : | : |\n",
        "|Y| : |B: |\n",
        "+---------+\n",
        "</pre>"
      ],
      "metadata": {
        "id": "79AoUwTxRaO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation and action space \n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXGys5B3PPWw",
        "outputId": "0f69955b-9a56-4892-c266-6742d0ffed23"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The observation space: Discrete(500)\n",
            "The action space: Discrete(6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Action space:\n",
        "\n",
        "As the figure shown above, there are **6 discrete deterministic actions**:\n",
        "\n",
        "- 0: move south\n",
        "\n",
        "- 1: move north\n",
        "\n",
        "- 2: move east\n",
        "\n",
        "- 3: move west\n",
        "\n",
        "- 4: pickup passenger\n",
        "\n",
        "- 5: drop off passenger\n",
        "\n",
        "## b. Observation space:\n",
        "\n",
        "According to the rule, there are `25` taxi positions, `5` possible locations of the passenger (including the case when the passenger is in the taxi), and `4` destination locations. => **25\\*5\\*4 = 500 states**\n",
        "\n",
        "Each state space is represented by the tuple: **(taxi_row, taxi_col, passenger_location, destination)**\n",
        "\n",
        "The state tuple can then be decoded with the `decode` method from integer with the following convention:\n",
        "\n",
        "Passenger locations:\n",
        "\n",
        "- 0: R(ed)\n",
        "\n",
        "- 1: G(reen)\n",
        "\n",
        "- 2: Y(ellow)\n",
        "\n",
        "- 3: B(lue)\n",
        "\n",
        "- 4: in taxi\n",
        "\n",
        "Destinations:\n",
        "\n",
        "- 0: R(ed)\n",
        "\n",
        "- 1: G(reen)\n",
        "\n",
        "- 2: Y(ellow)\n",
        "\n",
        "- 3: B(lue)\n",
        "\n",
        "## c. Rewards rules (Read carefully)\n",
        "\n",
        "- -1 per step unless other reward is triggered.\n",
        "\n",
        "- +20 delivering passenger.\n",
        "\n",
        "- -10 executing “pickup” and “drop-off” actions illegally."
      ],
      "metadata": {
        "id": "HYBZQbbmQco-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # Reset the state of the environment randomly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4rSJ34Hi7jo",
        "outputId": "1e682d9b-d157-491c-dd48-873e978ce1ee"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "283"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would apply MDL (Markov Decision Process) model into our training. The MDL suits for discrete, stochastic, sequential environments.\n",
        "\n",
        "<center>\n",
        " <img src=\"https://i.imgur.com/GeLizkh.png\" alt=\"mdp-model\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "pzzaONGPywB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carry out a **stochastic** chain of actions. This should be fun when watching in your own local terminal when observing the map's changes\n",
        "\n",
        "> Note: In the map, `yellow square` is our **taxi agent**, location (R/G/B/Y) of **passenger** is highlighted with `blue`, **destination** is `red`"
      ],
      "metadata": {
        "id": "DLu65vOvti8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # sample a random action from the list of available actions\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # perform this action on the environment\n",
        "    env.step(action)\n",
        "\n",
        "    # print the new state\n",
        "    map = env.render(mode='ansi')\n",
        "    print(map)\n",
        "\n",
        "# end this instance of the taxi environment\n",
        "env.close()"
      ],
      "metadata": {
        "id": "mLdRtl79pPGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edf4ba7-c65f-43cb-c8da-9b884e9dac41"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 1 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 2 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 3 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 4 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 5 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 6 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 7 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 8 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 9 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 10 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 11 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 12 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 13 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 14 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 15 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 16 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 17 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 18 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 19 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 20 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 21 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 22 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 23 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 24 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 25 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 26 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 27 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 28 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 29 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 30 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 31 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 32 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 33 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 34 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 35 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 36 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 37 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 38 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 39 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 40 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 41 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 42 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 43 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 44 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 45 out of 99\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 46 out of 99\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 47 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 48 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 49 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 50 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 51 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 52 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[42m_\u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 53 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[42m_\u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 54 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[42m_\u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 55 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 56 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[42m_\u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 57 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 58 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 59 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 60 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 61 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 62 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 63 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 64 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[42m_\u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 65 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[42m_\u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 66 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 67 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 68 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 69 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 70 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 71 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[42m_\u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 72 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 73 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 74 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 75 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[42m_\u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 76 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[42m_\u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 77 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[42m_\u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 78 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 79 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 80 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 81 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 82 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 83 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 84 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 85 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 86 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 87 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 88 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "step: 89 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 90 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 91 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 92 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 93 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "step: 94 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "step: 95 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 96 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "step: 97 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "step: 98 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "step: 99 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Training and Applying (Demo step-by-step)"
      ],
      "metadata": {
        "id": "CrZeA_DQ-BEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here comes the Q-Table\n",
        "\n",
        "Reference: [RL Gym OpenAI Tutorial](https://www.gocoder.one/blog/rl-tutorial-with-openai-gym)\n",
        "\n",
        "A Q-table is simply a look-up table storing values representing **the maximum expected future rewards** our agent can expect for a *certain action* in a *certain state* (known as `Q-values`). It will tell our agent that when it encounters a certain state, some actions are more likely than others to lead to higher rewards. It becomes a **'cheatsheet'** or you can comprehend it like a **'small database'** telling our agent what the best action to take is.\n",
        "\n",
        "The image below illustrates what our **Q-table** will look like:\n",
        "\n",
        "<center>\n",
        " <img src=\"https://i.imgur.com/e7yGkcl.png\" alt=\"q-table\">\n",
        "</center>\n",
        "\n",
        "- Each **row** corresponds to a unique `state` in the 'Taxi' environment\n",
        "- Each **column** corresponds to an `action` our agent can take\n",
        "- Each **cell** corresponds to the `Q-value` for that state-action pair - a higher Q-value means a higher maximum reward our agent can expect to get if it takes that action in that state. \n",
        "\n",
        "> Note: The `Q-values` could be achieved through **exploration** and **exploitation**. In other words, the `Q-tables` is constructed by learning the map."
      ],
      "metadata": {
        "id": "PSFffg2c3wUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how the `Q-values` is calculated. Well, with **Q-Learning algorithm**, the following is the formula: \n",
        "\n",
        "<center>\n",
        " <img src=\"https://i.imgur.com/NOQfLd7.png\" alt=\"Q-learning-formula\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "8yYs_nJF6Zsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo code:\n",
        "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))"
      ],
      "metadata": {
        "id": "cRw3HEp59gKY"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, initialize q-table with 2-d dimension array with 'state' and 'action'\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "qtable"
      ],
      "metadata": {
        "id": "hAoOCJTDt-fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b299a784-5b2f-423b-81bb-283b0ff29e3c"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: We set some value `epsilon` between 0 and 1. If `epsilon` is **0.7**, there is a **70%** chance that on this step our agent will explore instead of exploit. `epsilon` exponentially `decays` with each step, so that our agent explores less and less over time"
      ],
      "metadata": {
        "id": "k7bqpc4T9BM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "\n",
        "# hyperparameters to tune or for adjustment\n",
        "learning_rate = 0.9 # how easily the agent should accept new information over previously learnt information\n",
        "discount_rate = 0.8 # how much the agent should take into consideration the rewards it could receive in the future versus its immediate reward\n",
        "\n",
        "# exploration-exploitation tradeoff\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.005"
      ],
      "metadata": {
        "id": "YOj9S4mFt-X7"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training variables\n",
        "num_episodes = 1000\n",
        "max_steps = 99 # per episode"
      ],
      "metadata": {
        "id": "ZmKPqwHauEC6"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training phase\n",
        "\n",
        "> Note: Use the code in the comment if there is an error like `ValueError: not enough values to unpack (expected 5, got 4)` due to version compatibility. See the changes [here](https://stackoverflow.com/questions/73195438/openai-gyms-env-step-what-are-the-values)"
      ],
      "metadata": {
        "id": "ADiARGK7uIsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    print(\"======= Start episode %d =======\" % episode)\n",
        "    # reset the environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for s in range(max_steps):\n",
        "\n",
        "        # exploration-exploitation tradeoff\n",
        "        if random.uniform(0,1) < epsilon:\n",
        "            # Exploration\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploitation\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # take action and observe reward from the current action\n",
        "        #new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        new_state, reward, done, info = env.step(action) \n",
        "        print(\"New state: %d\" % new_state)\n",
        "        print(\"Reward: %d\" % reward)\n",
        "        print(\"Done: %r\" % done)\n",
        "        print(\"Info: %s\" % info)\n",
        "\n",
        "        # Q-learning algorithm\n",
        "        # Update the Q-table\n",
        "        qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "        # Update to our new state\n",
        "        state = new_state\n",
        "\n",
        "        # if done, finish episode\n",
        "        #if terminated == True or truncated == True:\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    # Decrease epsilon\n",
        "    epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "print(f\"TRAINING PROCESS COMPLETED OVER {num_episodes} EPISODES\")"
      ],
      "metadata": {
        "id": "6eJLN3f9qPy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4041753f-b8ce-49cc-d8e9-42e61bbea87c"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 910 =======\n",
            "New state: 371\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 271\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 251\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 911 =======\n",
            "New state: 227\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 912 =======\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 913 =======\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 914 =======\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 915 =======\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 916 =======\n",
            "New state: 251\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 917 =======\n",
            "New state: 148\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 248\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 918 =======\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 919 =======\n",
            "New state: 124\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 224\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 244\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 920 =======\n",
            "New state: 251\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 921 =======\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 922 =======\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -10\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 923 =======\n",
            "New state: 313\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 413\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 313\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 213\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 233\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 253\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 273\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 373\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 473\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 497\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 924 =======\n",
            "New state: 124\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 224\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 244\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 925 =======\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 926 =======\n",
            "New state: 232\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 927 =======\n",
            "New state: 368\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 268\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 248\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 928 =======\n",
            "New state: 473\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 377\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 929 =======\n",
            "New state: 371\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 271\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 251\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 930 =======\n",
            "New state: 473\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 377\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 931 =======\n",
            "New state: 27\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 127\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 227\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 932 =======\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 933 =======\n",
            "New state: 224\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 244\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 934 =======\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 935 =======\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 936 =======\n",
            "New state: 69\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 49\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 149\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 249\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 937 =======\n",
            "New state: 249\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 938 =======\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 939 =======\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 940 =======\n",
            "New state: 233\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 253\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 273\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 373\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 473\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 377\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 941 =======\n",
            "New state: 281\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 261\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 241\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 21\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 942 =======\n",
            "New state: 111\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -10\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 943 =======\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 944 =======\n",
            "New state: 344\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 244\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 144\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 44\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 64\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 945 =======\n",
            "New state: 127\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 227\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 946 =======\n",
            "New state: 387\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 947 =======\n",
            "New state: 361\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 261\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 241\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 948 =======\n",
            "New state: 206\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 226\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 949 =======\n",
            "New state: 212\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 232\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 950 =======\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 951 =======\n",
            "New state: 486\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 386\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 286\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 186\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 952 =======\n",
            "New state: 104\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 124\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 224\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 244\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 953 =======\n",
            "New state: 48\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 148\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 248\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 954 =======\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 955 =======\n",
            "New state: 449\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 449\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 349\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 329\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 956 =======\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 957 =======\n",
            "New state: 127\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 227\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 127\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 227\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 958 =======\n",
            "New state: 362\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 262\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 959 =======\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 960 =======\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 961 =======\n",
            "New state: 206\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 226\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 962 =======\n",
            "New state: 141\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 241\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 963 =======\n",
            "New state: 243\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 223\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 964 =======\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 377\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 965 =======\n",
            "New state: 206\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 226\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 966 =======\n",
            "New state: 342\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 967 =======\n",
            "New state: 112\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 212\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 232\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 968 =======\n",
            "New state: 269\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 249\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 969 =======\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 970 =======\n",
            "New state: 246\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 146\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 166\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 66\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 86\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 98\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 198\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 178\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 158\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 971 =======\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 972 =======\n",
            "New state: 431\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 331\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 231\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 973 =======\n",
            "New state: 62\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 42\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 142\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 974 =======\n",
            "New state: 243\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 223\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 975 =======\n",
            "New state: 321\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 976 =======\n",
            "New state: 347\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 977 =======\n",
            "New state: 343\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 243\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 223\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 978 =======\n",
            "New state: 111\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 211\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 311\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 411\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 419\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 319\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 979 =======\n",
            "New state: 284\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 980 =======\n",
            "New state: 322\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 342\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 981 =======\n",
            "New state: 223\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 982 =======\n",
            "New state: 8\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 8\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 28\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 128\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 983 =======\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 984 =======\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 985 =======\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 986 =======\n",
            "New state: 234\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 254\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 274\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 374\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 474\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 478\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 378\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 278\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 258\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 238\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 987 =======\n",
            "New state: 283\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 263\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 243\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 223\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 123\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 103\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 3\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 19\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 119\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 219\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 239\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 259\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 988 =======\n",
            "New state: 247\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 267\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 287\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 187\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 87\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 99\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 199\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 299\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 279\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 379\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 479\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 475\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 989 =======\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 990 =======\n",
            "New state: 221\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 121\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 101\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 1\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 17\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 117\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 137\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 991 =======\n",
            "New state: 364\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 264\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 992 =======\n",
            "New state: 477\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 377\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 397\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 993 =======\n",
            "New state: 128\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 228\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 208\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 308\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 408\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 416\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 316\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 216\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 994 =======\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 995 =======\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 996 =======\n",
            "New state: 352\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 252\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 272\n",
            "Reward: -10\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 372\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 472\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 476\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 376\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 997 =======\n",
            "New state: 164\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 184\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 84\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 96\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 196\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 296\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 276\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 256\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 236\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 136\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 116\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 16\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 0\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 998 =======\n",
            "New state: 342\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 242\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 222\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 122\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 102\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 2\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 18\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 118\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 218\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 318\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 418\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 410\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            "======= Start episode 999 =======\n",
            "New state: 389\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 489\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 469\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 369\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 269\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 249\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 229\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 209\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 309\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 409\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 417\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 317\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 217\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 237\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 257\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 277\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 297\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 197\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 97\n",
            "Reward: -1\n",
            "Done: False\n",
            "Info: {'prob': 1.0}\n",
            "New state: 85\n",
            "Reward: 20\n",
            "Done: True\n",
            "Info: {'prob': 1.0}\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "TRAINING PROCESS COMPLETED OVER 1000 EPISODES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: Use the code in the comment if there is an error like `ValueError: not enough values to unpack (expected 5, got 4)` due to version compatibility. See the changes [here](https://stackoverflow.com/questions/73195438/openai-gyms-env-step-what-are-the-values)"
      ],
      "metadata": {
        "id": "OrNgpEtwt7pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# watch trained agent\n",
        "\n",
        "state = env.reset()\n",
        "# terminated = False\n",
        "# truncated = False\n",
        "done = False\n",
        "rewards = 0\n",
        "\n",
        "for s in range(max_steps):\n",
        "\n",
        "    print(f\"TRAINED AGENT\")\n",
        "    print(\"Step {}\".format(s+1))\n",
        "\n",
        "    action = np.argmax(qtable[state,:])\n",
        "    #new_state, reward, terminated, truncated, info = env.step(action)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    rewards += reward # Accumulate the reward\n",
        "\n",
        "    # Show the current state with action\n",
        "    map = env.render(mode='ansi')\n",
        "    print(map)\n",
        "\n",
        "    print(f\"score: {rewards}\")\n",
        "    state = new_state\n",
        "\n",
        "    #if terminated == True or truncated == True:\n",
        "    if done == True:\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "VUwV8Qo9rMPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8761d032-455a-4b1c-a11b-649c26d676b3"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINED AGENT\n",
            "Step 1\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "score: -1\n",
            "TRAINED AGENT\n",
            "Step 2\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "score: -2\n",
            "TRAINED AGENT\n",
            "Step 3\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "score: -3\n",
            "TRAINED AGENT\n",
            "Step 4\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "score: -4\n",
            "TRAINED AGENT\n",
            "Step 5\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "score: -5\n",
            "TRAINED AGENT\n",
            "Step 6\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "score: -6\n",
            "TRAINED AGENT\n",
            "Step 7\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "score: -7\n",
            "TRAINED AGENT\n",
            "Step 8\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "score: -8\n",
            "TRAINED AGENT\n",
            "Step 9\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "score: -9\n",
            "TRAINED AGENT\n",
            "Step 10\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "score: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation"
      ],
      "metadata": {
        "id": "VjTTqbLl-ZaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Build model as function for reuse "
      ],
      "metadata": {
        "id": "JxFpb07vFvIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes, max_steps, learning_rate, discount_rate, epsilon, decay_rate):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        # epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "        for step in range(max_steps):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action) \n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all"
      ],
      "metadata": {
        "id": "-FzxlSqy_0ST"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, rewards_all = q_learning(env, num_episodes, max_steps, learning_rate, discount_rate, epsilon, decay_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7nzKI8nBnfk",
        "outputId": "f6654540-3068-4473-e792-e616f3495e3e"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 999 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4inATrPBp6r",
        "outputId": "febd604c-58f5-472d-f73c-c77c48659693"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [-4.28971922, -4.16190225, -4.13513438, -4.16190225, -1.6445568 ,\n",
              "        -9.        ],\n",
              "       [-3.72344119, -3.47996664, -3.71689872, -3.47996664,  3.192     ,\n",
              "        -9.        ],\n",
              "       ...,\n",
              "       [-3.14630078,  4.09758312, -3.14630078, -3.13929922, -9.        ,\n",
              "        -9.        ],\n",
              "       [-3.47996664, -3.51038543, -3.51333323, -3.52896892, -9.        ,\n",
              "        -9.        ],\n",
              "       [-1.638     , -1.638     , -1.638     , 11.97      , -9.        ,\n",
              "        -9.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Create `play` function to watch trained agent: Reset the environment and do the 'watch the trained agent' task above"
      ],
      "metadata": {
        "id": "fXHfXVVKF3MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "metadata": {
        "id": "khBxTmAsBuYV"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Show performance: If reward is positve, we win this case"
      ],
      "metadata": {
        "id": "FVlWx148GFw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, q_table, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, q_table)\n",
        "\n",
        "        if total_reward > 0: \n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "metadata": {
        "id": "AJYQaMiHCbce"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjRy7_DECfbT",
        "outputId": "bbb38530-f2e0-4af8-eeeb-832858bd0a6c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 989/1000\n",
            "Average number of steps: 13.180990899898887\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}